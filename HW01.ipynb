{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7560b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isna().sum()\n",
    "\n",
    "# Display the number of missing values for each column\n",
    "print(missing_values)\n",
    "\n",
    "#This code will print the number of missing values for each column in the DataFrame.\n",
    "#If you run this code, you should see the output with the number of missing values for each column in the dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681ed3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the dataset\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Get the number of rows and columns\n",
    "num_rows, num_cols = df.shape\n",
    "\n",
    "# Print the number of rows and columns\n",
    "print(f\"The dataset has {num_rows} rows and {num_cols} columns.\")\n",
    "\n",
    "The dataset has 400 rows and 8 columns.\n",
    "# This output indicates that the dataset has 400 rows and 8 columns.\n",
    "\n",
    "\n",
    "Definition: Observations refer to the individual rows in a dataset.\n",
    "Each observation represents a single instance or record of data and contains values for all the variables or attributes defined in the dataset.\n",
    "\n",
    "Contextual Example: If you're working with a dataset of villagers from a game, each observation would be one villager's record.\n",
    "This record would include all the details about that villager, such as their name, species, birthday, and other attributes.\n",
    "\n",
    "# Variables\n",
    "\n",
    "Definition: Variables refer to the columns in a dataset.\n",
    "Each variable represents a specific attribute or characteristic that is measured or recorded for each observation.\n",
    "Variables provide the different dimensions along which observations are analyzed.\n",
    "\n",
    "\n",
    "Contextual Example: In the same dataset of villagers, variables might include name, species, birthday, gender, etc. Each variable captures a different aspect of the villager’s data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcbb9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.\n",
    "\n",
    "To provide simple summaries of the columns in a dataset, you can use various methods available in pandas. Common methods include:\n",
    "\n",
    "df.describe(): Provides summary statistics for numeric columns, such as mean, standard deviation, minimum, and maximum values.\n",
    "    \n",
    "df.info(): Gives a concise summary of the DataFrame, including the number of non-null values and data types for each column.\n",
    "    \n",
    "df.head(): Displays the first few rows of the DataFrame, which can be useful to get a quick look at the data.\n",
    "    \n",
    "df.value_counts(): Shows the frequency of unique values for categorical columns.\n",
    "    \n",
    "import pandas as pd\n",
    "\n",
    "# URL of the dataset\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Get summary statistics for numeric columns\n",
    "print(\"Summary statistics for numeric columns:\")\n",
    "print(df.describe(include='all'))  # Use include='all' to summarize all columns\n",
    "\n",
    "# Get a concise summary of the DataFrame\n",
    "print(\"\\nDataFrame info:\")\n",
    "print(df.info())\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"\\nFirst few rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Show the frequency of unique values for a categorical column\n",
    "# For example, summarizing the 'species' column\n",
    "print(\"\\nFrequency of unique values in 'species':\")\n",
    "print(df['species'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c4d979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.\n",
    "\n",
    "Steps to Analyze the Dataset\n",
    "1.Load the Titanic Dataset\n",
    "2.Compare df.shape and df.describe()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Print the shape of the dataset\n",
    "print(\"Dataset shape (rows, columns):\", df.shape)\n",
    "\n",
    "# Print a summary of statistics for numeric columns\n",
    "print(\"\\nSummary statistics for numeric columns:\")\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "Analysis of Discrepancies\n",
    "(a) Number of Columns Analyzed\n",
    "\n",
    "df.shape: Reports the total number of columns in the dataset.\n",
    "    \n",
    "(rows, columns) = df.shape\n",
    "\n",
    "df.describe(): By default, df.describe() analyzes only numeric columns unless include='all' is specified.\n",
    "This means that if your dataset includes non-numeric variables (e.g., categorical data like 'sex', 'embarked', 'class' in the Titanic dataset),\n",
    "these columns will not be included in the summary statistics provided by df.describe() unless you explicitly include them.\n",
    "\n",
    "\n",
    "df.describe(include='all')  # Include all columns, regardless of type\n",
    "\n",
    "\n",
    "(b) Count of Non-Null Values in Numeric Columns\n",
    "\n",
    "df.describe(): Shows the count of non-null values for each numeric column under the \"count\" column.\n",
    "This count reflects the number of non-null (i.e., non-missing) entries in that column.\n",
    "If a numeric column has missing values, the count will be less than the total number of rows.\n",
    "\n",
    "Discrepancy Explanation: The discrepancy between df.shape[0] (total rows) and the \"count\" values in df.describe() indicates the presence of missing values in numeric columns.\n",
    "For non-numeric columns, missing values are not counted in the df.describe() summary,\n",
    "hence the column might not be listed if only numeric columns are considered.\n",
    "\n",
    "\n",
    "Example Output and Explanation\n",
    "Assuming you run the above code with the Titanic dataset:\n",
    "    \n",
    "Dataset shape (rows, columns): (714, 7)\n",
    "\n",
    "Summary statistics for numeric columns:\n",
    "         age        fare\n",
    "count  714.000000  714.000000\n",
    "mean    29.699118   32.204208\n",
    "std     14.526497   49.693429\n",
    "min      0.420000    0.000000\n",
    "25%     20.000000    7.910400\n",
    "50%     28.000000   14.454200\n",
    "75%     38.000000   31.275000\n",
    "max     80.000000  512.329200\n",
    "\n",
    "\n",
    "Number of Columns Analyzed: If df.shape indicates 7 columns, but df.describe() (when not using include='all') shows statistics for only 2 columns (age and fare),\n",
    "it means the other columns are non-numeric (e.g., sex, class, embarked, who, alone).\n",
    "\n",
    "Count Discrepancy: The \"count\" value for age and fare is less than 714, indicating that these columns have missing values.\n",
    "For instance, if age has 714 entries and only 714 are counted in the description, it means there are no missing values in age.\n",
    "But if it is less, then there are missing values in age.\n",
    "\n",
    "#Conclusion\n",
    "\n",
    "Non-Numeric Variables: Columns with non-numeric data are excluded by default in df.describe() unless include='all' is used.\n",
    "    \n",
    "Missing Values: Discrepancies in the \"count\" column in df.describe() compared to the total number of rows reported by df.shape highlight missing values in numeric columns.\n",
    "This approach helps in understanding how missing data and different types of variables are represented and summarized in a dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6ba7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.\n",
    "\n",
    "To understand the difference between an \"attribute\" and a \"method\" in Python, let's break down their roles and usages, and then provide a paraphrased summarization.\n",
    "\n",
    "### Attributes vs. Methods\n",
    "\n",
    "#### Attributes\n",
    "- **Definition**: An attribute is a property or characteristic of an object. In pandas, attributes are accessed directly without parentheses `()`. They provide information about the object or hold data.\n",
    "- **Example**: `df.shape` is an attribute of the DataFrame `df`. It provides the dimensions of the DataFrame (number of rows and columns) as a tuple, but it doesn’t perform any actions or computations.\n",
    "  ```python\n",
    "  # Accessing an attribute\n",
    "  dimensions = df.shape  # Returns a tuple (rows, columns)\n",
    "  ```\n",
    "\n",
    "#### Methods\n",
    "- **Definition**: A method is a function that belongs to an object. Methods are called with parentheses `()` and can perform actions or computations, potentially modifying the object or returning computed results.\n",
    "- **Example**: `df.describe()` is a method of the DataFrame `df`. It computes and returns summary statistics for numeric columns. Methods often perform operations and may require arguments to specify what actions to take.\n",
    "  ```python\n",
    "  # Calling a method\n",
    "  summary_stats = df.describe()  # Returns summary statistics of numeric columns\n",
    "  ```\n",
    "\n",
    "### Paraphrased Summarization\n",
    "\n",
    "**Attributes** are like data fields or properties that provide direct information about an object and are accessed without parentheses. They simply give you the current state or characteristics of the object. For instance, `df.shape` tells you the dimensions of the DataFrame directly.\n",
    "\n",
    "**Methods**, on the other hand, are functions associated with an object that perform actions or computations. They require parentheses to be executed and can return results or modify the object. For example, `df.describe()` calculates and returns summary statistics of the DataFrame's numeric columns.\n",
    "\n",
    "In essence, attributes are used to access existing data about an object, while methods are used to perform actions or retrieve computed data based on the object's state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb002c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.\n",
    "\n",
    "\n",
    "### 1. **Count**\n",
    "- **Definition**: The number of non-null (non-missing) values in the column.\n",
    "- **Usage**: Indicates how many entries are present in the dataset for that particular variable. It helps to understand the extent of missing data.\n",
    "\n",
    "### 2. **Mean**\n",
    "- **Definition**: The average value of the numeric column.\n",
    "- **Calculation**: Computed as the sum of all values divided by the number of values.\n",
    "  \\[\n",
    "  \\text{Mean} = \\frac{\\sum \\text{Values}}{\\text{Number of Values}}\n",
    "  \\]\n",
    "- **Usage**: Provides a measure of central tendency, showing the average value of the data.\n",
    "\n",
    "### 3. **Standard Deviation (std)**\n",
    "- **Definition**: A measure of the amount of variation or dispersion in the values of the column.\n",
    "- **Calculation**: The square root of the variance, where variance is the average of the squared differences from the mean.\n",
    "  \\[\n",
    "  \\text{Standard Deviation} = \\sqrt{\\frac{\\sum (x_i - \\text{Mean})^2}{N}}\n",
    "  \\]\n",
    "- **Usage**: Shows how spread out the values are around the mean. A higher standard deviation indicates more variability.\n",
    "\n",
    "### 4. **Minimum (min)**\n",
    "- **Definition**: The smallest value in the column.\n",
    "- **Usage**: Provides the lowest data point in the dataset, indicating the lower bound of the data range.\n",
    "\n",
    "### 5. **25th Percentile (25%)**\n",
    "- **Definition**: The value below which 25% of the data falls.\n",
    "- **Calculation**: Also known as the first quartile (Q1). It is the value at the 25th percentile of the data distribution.\n",
    "- **Usage**: Provides insight into the lower distribution of data, helping to understand the spread of the lowest quarter of the data.\n",
    "\n",
    "### 6. **50th Percentile (50%)**\n",
    "- **Definition**: The median value of the column, where 50% of the data falls below this value.\n",
    "- **Calculation**: The middle value when the data is sorted in ascending order. If there is an even number of observations, it is the average of the two middle values.\n",
    "- **Usage**: Represents the central point of the data distribution. It divides the data into two equal halves.\n",
    "\n",
    "### 7. **75th Percentile (75%)**\n",
    "- **Definition**: The value below which 75% of the data falls.\n",
    "- **Calculation**: Also known as the third quartile (Q3). It is the value at the 75th percentile of the data distribution.\n",
    "- **Usage**: Provides insight into the upper distribution of data, helping to understand the spread of the highest quarter of the data.\n",
    "\n",
    "### 8. **Maximum (max)**\n",
    "- **Definition**: The largest value in the column.\n",
    "- **Usage**: Shows the highest data point in the dataset, indicating the upper bound of the data range.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Count**: Number of non-null values.\n",
    "- **Mean**: Average value.\n",
    "- **Standard Deviation (std)**: Measure of dispersion.\n",
    "- **Minimum (min)**: Smallest value.\n",
    "- **25th Percentile (25%)**: Value below which 25% of data falls.\n",
    "- **50th Percentile (50%)**: Median value, middle of the data.\n",
    "- **75th Percentile (75%)**: Value below which 75% of data falls.\n",
    "- **Maximum (max)**: Largest value.\n",
    "\n",
    "These statistics help summarize the distribution and central tendencies of numeric variables in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef98b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.\n",
    "\n",
    "\n",
    "### Use Case for `df.dropna()`\n",
    "\n",
    "**Scenario**: You have a dataset where some rows have missing values in one or more columns, but you want to retain the columns that have a lot of useful data. \n",
    "\n",
    "**Example Use Case**:\n",
    "Imagine you have a dataset of customer reviews for products, with columns such as `review_id`, `product_id`, `review_text`, `rating`, and `date`. Suppose many rows are missing the `rating` value, but the `review_text` and `date` columns are complete.\n",
    "\n",
    "In this case, you might want to use `df.dropna()` to remove rows where the `rating` is missing, since `rating` is crucial for your analysis but you don’t want to lose valuable data from `review_text` and `date`. By using `df.dropna()`, you ensure that only rows with complete information for the `rating` column are kept, which is important for the quality of your analysis or model.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame\n",
    "data = {'review_id': [1, 2, 3, 4],\n",
    "        'product_id': [101, 102, 103, 104],\n",
    "        'review_text': ['Good', 'Bad', 'Average', 'Excellent'],\n",
    "        'rating': [5, None, 3, None],\n",
    "        'date': ['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Drop rows where 'rating' is missing\n",
    "df_cleaned = df.dropna(subset=['rating'])\n",
    "print(df_cleaned)\n",
    "```\n",
    "\n",
    "**Justification**: Using `df.dropna()` here ensures that you retain only those rows where the `rating` column is complete, which is crucial for analysis. This method is preferred when the missing data is specific to certain rows but the columns themselves are useful.\n",
    "\n",
    "### Opposite Use Case for `del df['col']`\n",
    "\n",
    "**Scenario**: You have a dataset where one of the columns has a high percentage of missing values and is not critical for your analysis. \n",
    "\n",
    "**Example Use Case**:\n",
    "Suppose you have a dataset with various attributes of customers, including `customer_id`, `age`, `gender`, `income`, and `phone_number`. If the `phone_number` column has a significant number of missing values and is not required for your analysis, you might prefer to remove this column entirely rather than drop rows.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame\n",
    "data = {'customer_id': [1, 2, 3, 4],\n",
    "        'age': [25, 30, None, 22],\n",
    "        'gender': ['F', 'M', 'F', 'M'],\n",
    "        'income': [50000, 60000, None, 45000],\n",
    "        'phone_number': [None, None, None, None]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Drop the 'phone_number' column as it has no useful data\n",
    "del df['phone_number']\n",
    "print(df)\n",
    "```\n",
    "\n",
    "**Justification**: Using `del df['col']` is appropriate here as the column has no data and isn't needed for the analysis. Removing it can simplify the dataset and reduce unnecessary data processing.\n",
    "\n",
    "### Combining `del df['col']` and `df.dropna()`\n",
    "\n",
    "**Scenario**: You might use both methods when you want to first remove columns with too much missing data and then handle remaining rows with missing values.\n",
    "\n",
    "**Importance**:\n",
    "1. **Efficient Cleanup**: Removing irrelevant or excessively incomplete columns first with `del df['col']` ensures that you’re not processing unnecessary data and focusing on the columns that matter.\n",
    "2. **Focused Row Removal**: After cleaning up columns, you can then apply `df.dropna()` to remove rows with missing values in the columns that remain, ensuring that the data you keep is as complete and useful as possible.\n",
    "\n",
    "**Example**:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame\n",
    "data = {'id': [1, 2, 3, 4],\n",
    "        'feature1': [1, None, 3, 4],\n",
    "        'feature2': [None, None, 2, 3],\n",
    "        'feature3': [5, 6, None, None]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Remove columns with too many missing values (e.g., more than 50% missing)\n",
    "threshold = len(df) * 0.5\n",
    "df = df.dropna(axis=1, thresh=threshold)\n",
    "\n",
    "# Step 2: Remove rows with remaining missing values\n",
    "df = df.dropna()\n",
    "\n",
    "print(\"After cleaning:\")\n",
    "print(df)\n",
    "```\n",
    "\n",
    "**Before and After Report**:\n",
    "\n",
    "- **Before**: The dataset might have columns with excessive missing values and rows with missing values in key columns.\n",
    "- **After**: The dataset will have removed columns that were mostly missing and cleaned up rows to ensure only complete data is retained.\n",
    "\n",
    "**Justification**: This approach ensures you maintain the dataset’s integrity by focusing on meaningful columns and ensuring the remaining data is complete. This two-step process of column and row removal is practical for managing and analyzing datasets with missing values efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6797937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.\n",
    "\n",
    "\n",
    "### Understanding `df.groupby(\"col1\")[\"col2\"].describe()`\n",
    "\n",
    "The `df.groupby(\"col1\")[\"col2\"].describe()` method provides a summary of statistics for `col2`, grouped by unique values in `col1`. This helps in understanding how the statistics of `col2` vary across different groups in `col1`.\n",
    "\n",
    "**Example with the Titanic Dataset:**\n",
    "\n",
    "1. **Load the Titanic Data**:\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "\n",
    "   # URL of the Titanic dataset\n",
    "   url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "   \n",
    "   # Load the dataset\n",
    "   df = pd.read_csv(url)\n",
    "   ```\n",
    "\n",
    "2. **Example Analysis**:\n",
    "   Let’s analyze the `age` column grouped by `class`:\n",
    "   ```python\n",
    "   group_summary = df.groupby(\"class\")[\"age\"].describe()\n",
    "   print(group_summary)\n",
    "   ```\n",
    "\n",
    "**Explanation**: This will provide summary statistics (like count, mean, std, min, etc.) for the `age` column within each `class` group (`First`, `Second`, `Third`).\n",
    "\n",
    "### Why `df.describe()` and `df.groupby(\"col1\")[\"col2\"].describe()` Differ in `count`\n",
    "\n",
    "- **`df.describe()`**: Provides the count of non-null values for each column across the entire DataFrame. If a column has missing values, its count reflects the number of non-missing entries.\n",
    "\n",
    "- **`df.groupby(\"col1\")[\"col2\"].describe()`**: Provides the count of non-null values within each group. If some groups have missing values in `col2`, the count will vary by group. This helps to understand missing data distribution across different groups, rather than just across the entire dataset.\n",
    "\n",
    "### Error Handling and Troubleshooting\n",
    "\n",
    "1. **Forget to Include `import pandas as pd`**\n",
    "\n",
    "   **Error**: `NameError: name 'pd' is not defined`\n",
    "   - **ChatBot**: Can guide you to include the necessary import statement.\n",
    "   - **Google**: Search for the error, which will suggest adding the import statement.\n",
    "\n",
    "   **Opinion**: Using a ChatBot is helpful to directly get advice on fixing the import issue. Google search is also effective but may involve sifting through various sources.\n",
    "\n",
    "2. **Mistype \"titanic.csv\" as \"titanics.csv\"**\n",
    "\n",
    "   **Error**: `FileNotFoundError: [Errno 2] No such file or directory: 'titanics.csv'`\n",
    "   - **ChatBot**: Can suggest checking and correcting the file name.\n",
    "   - **Google**: Search for `FileNotFoundError`, which typically suggests checking file paths.\n",
    "\n",
    "   **Opinion**: ChatBot provides a more interactive and direct way to address the typo compared to searching through generic solutions.\n",
    "\n",
    "3. **Typos in URL**\n",
    "\n",
    "   **Example Typos and Errors**:\n",
    "   - **Typo Example**: `https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanicss.csv`\n",
    "   - **Error**: `FileNotFoundError` or `404 Not Found`\n",
    "   \n",
    "   - **ChatBot**: Can guide you to verify and correct the URL.\n",
    "   - **Google**: Search for `404 Not Found` or similar errors.\n",
    "\n",
    "   **Opinion**: Both ChatBot and Google are useful, but ChatBot can provide immediate feedback and specific suggestions.\n",
    "\n",
    "4. **Using DataFrame Before Assignment**\n",
    "\n",
    "   **Error**: `NameError: name 'df' is not defined`\n",
    "   - **ChatBot**: Can suggest checking the variable name and assignment.\n",
    "   - **Google**: Search for `NameError` to understand variable scope issues.\n",
    "\n",
    "   **Opinion**: ChatBot offers contextual help for code errors directly. Google search also helps but might require broader problem-solving.\n",
    "\n",
    "5. **Forget One Parenthesis**\n",
    "\n",
    "   **Error**: `SyntaxError: unexpected EOF while parsing`\n",
    "   - **ChatBot**: Can suggest checking for syntax issues, like missing parentheses.\n",
    "   - **Google**: Search for `SyntaxError` to find common causes and fixes.\n",
    "\n",
    "   **Opinion**: ChatBot can quickly pinpoint syntax errors, while Google may require browsing through general syntax error solutions.\n",
    "\n",
    "6. **Mistyped Chained Functions**\n",
    "\n",
    "   **Errors**: \n",
    "   - `AttributeError: 'DataFrame' object has no attribute 'group_by'`\n",
    "   - `AttributeError: 'Series' object has no attribute 'describle'`\n",
    "\n",
    "   **ChatBot**: Can help identify the correct function names and syntax.\n",
    "   **Google**: Search for specific errors to find common mistakes in function names.\n",
    "\n",
    "   **Opinion**: ChatBot is useful for direct corrections, while Google provides general troubleshooting advice.\n",
    "\n",
    "7. **Incorrect Column Name in `groupby`**\n",
    "\n",
    "   **Error**: `KeyError: 'Sex'` or similar errors indicating the column is not found.\n",
    "   - **ChatBot**: Can help check the correct column names.\n",
    "   - **Google**: Search for `KeyError` to understand how to fix column name issues.\n",
    "\n",
    "   **Opinion**: ChatBot provides specific advice on column names, while Google offers general error solutions.\n",
    "\n",
    "8. **Column Name Not in Quotes**\n",
    "\n",
    "   **Errors**: \n",
    "   - `NameError: name 'sex' is not defined`\n",
    "   - `NameError: name 'age' is not defined`\n",
    "\n",
    "   **ChatBot**: Can highlight the need for quotes around string literals.\n",
    "   **Google**: Search for `NameError` related to string handling.\n",
    "\n",
    "   **Opinion**: ChatBot provides targeted advice for syntax issues, whereas Google might require looking through multiple sources.\n",
    "\n",
    "### Removing Missing Data\n",
    "\n",
    "**Example with Titanic Dataset**:\n",
    "\n",
    "1. **Load the Titanic Data**:\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "   df = pd.read_csv(url)\n",
    "   ```\n",
    "\n",
    "2. **Initial Data Summary**:\n",
    "   ```python\n",
    "   print(df.describe())\n",
    "   ```\n",
    "\n",
    "3. **Remove Missing Data**:\n",
    "   ```python\n",
    "   # Drop columns with more than 50% missing values\n",
    "   threshold = len(df) * 0.5\n",
    "   df_cleaned = df.dropna(axis=1, thresh=threshold)\n",
    "   \n",
    "   # Drop rows with missing values in remaining columns\n",
    "   df_cleaned = df_cleaned.dropna()\n",
    "   \n",
    "   # Print results\n",
    "   print(\"Before cleaning:\")\n",
    "   print(df.describe())\n",
    "   print(\"\\nAfter cleaning:\")\n",
    "   print(df_cleaned.describe())\n",
    "   ```\n",
    "\n",
    "**Justification**: Removing columns with excessive missing values ensures only useful columns are retained. Dropping rows ensures that all remaining data is complete. This approach balances data quality and quantity, making the dataset suitable for analysis.\n",
    "\n",
    "**Before and After Report**:\n",
    "- **Before**: The `count` of non-null values in various columns may be lower due to missing data.\n",
    "- **After**: `df_cleaned` will have a higher count of non-null values, reflecting a dataset with more complete entries. \n",
    "\n",
    "In summary, using `df.dropna()` and `del df['col']` effectively cleans up the dataset by addressing both column and row-level missing data issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a8e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.\n",
    "\n",
    "Yes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
