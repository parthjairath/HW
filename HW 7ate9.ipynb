{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "248b8704",
   "metadata": {},
   "source": [
    "# Answer 1\n",
    "\n",
    "A Simple Linear Regression (SLR) model is a statistical approach used to describe the relationship between two continuous variables: a predictor (independent variable) and an outcome (dependent variable). The goal of SLR is to find a linear equation that best represents how changes in the predictor variable are associated with changes in the outcome variable.\n",
    "\n",
    "Components of the Model\n",
    "1. Predictor Variable (X): This is the independent variable that we use to predict the value of the outcome. For instance, it could represent a factor like hours of study time.\n",
    "\n",
    "2. Outcome Variable (Y): This is the dependent variable that we are trying to predict. In the study example, it might represent test scores.\n",
    "\n",
    "3. Intercept (\n",
    "β\n",
    "0\n",
    "​\t\n",
    " ): The intercept is the value of the outcome variable when the predictor variable is zero. In the linear equation, it represents the starting point of the line on the Y-axis.\n",
    " \n",
    "4. Slope (\n",
    "β\n",
    "1\n",
    "​\t\n",
    " ): The slope is a coefficient that represents the rate of change in the outcome variable for every unit change in the predictor variable. It determines the steepness and direction (positive or negative) of the line.\n",
    " \n",
    "5. Error Term (\n",
    "ϵ): This represents random variations or deviations from the line due to unobserved factors. It assumes that errors are normally distributed with a mean of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e646717",
   "metadata": {},
   "source": [
    "# Answer 2\n",
    "\n",
    "Step-by-Step Process\n",
    "\n",
    "1. Simulate the Dataset: We'll generate predictor X and outcome Y based on a linear relationship with added noise.\n",
    "\n",
    "2. Fit the Model: Using statsmodels.formula.api, we'll fit a simple linear regression model to the data.\n",
    "\n",
    "3. Visualize the Results: We'll visualize the fitted regression line alongside the observed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6395ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Set parameters\n",
    "np.random.seed(0)\n",
    "beta_0 = 5        # Intercept\n",
    "beta_1 = 2.5      # Slope\n",
    "n_samples = 100   # Number of data points\n",
    "error_std = 1.5   # Standard deviation of the error term\n",
    "\n",
    "# Generate predictor variable (X)\n",
    "X = np.linspace(0, 10, n_samples)\n",
    "\n",
    "# Generate normally distributed error term\n",
    "error = np.random.normal(0, error_std, n_samples)\n",
    "\n",
    "# Calculate outcome variable (Y) based on the SLR model\n",
    "Y = beta_0 + beta_1 * X + error\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "data = pd.DataFrame({'X': X, 'Y': Y})\n",
    "\n",
    "# Fit the Simple Linear Regression model\n",
    "model = smf.ols('Y ~ X', data=data).fit()\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Plot the observed data and fitted line\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data['X'], data['Y'], color='blue', label='Observed Data')\n",
    "plt.plot(data['X'], model.predict(data), color='red', label='Fitted Line')\n",
    "plt.xlabel('Predictor (X)')\n",
    "plt.ylabel('Outcome (Y)')\n",
    "plt.title('Fitted Simple Linear Regression Model')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3012da12",
   "metadata": {},
   "source": [
    "# Answer 3\n",
    "\n",
    "Explanation of the Difference Between the Two Lines\n",
    "\n",
    "1. Theoretical Line (True Model, Green Dashed Line):\n",
    "This line represents the true relationship between \n",
    "X\n",
    "X and \n",
    "Y\n",
    "Y based on the parameters we set for the simulation: \n",
    "β \n",
    "0\n",
    "​\t\n",
    " =5 (intercept) and β \n",
    "1\n",
    "​\t\n",
    " =2.5 (slope).\n",
    "It’s generated without any influence from the specific random error in our simulated data points, showing the ideal or “population” relationship.\n",
    "\n",
    "\n",
    "2. Fitted Line (Estimated Model, Red Line):\n",
    "This line represents the linear relationship estimated by the model, calculated from the sample data we generated.\n",
    "Because it’s based on the actual data points, it incorporates random sampling variation, meaning it reflects the observed relationship, which may slightly differ from the true model.\n",
    "The difference between this line and the theoretical line is due to random sampling variation — each time we simulate new data, the error term \n",
    "ϵ\n",
    "ϵ would vary, leading to slight differences in the estimated intercept and slope values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4507eb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Set parameters\n",
    "np.random.seed(0)\n",
    "beta_0 = 5        # Intercept (true value)\n",
    "beta_1 = 2.5      # Slope (true value)\n",
    "n_samples = 100   # Number of data points\n",
    "error_std = 1.5   # Standard deviation of the error term\n",
    "\n",
    "# Generate predictor variable (X)\n",
    "X = np.linspace(0, 10, n_samples)\n",
    "\n",
    "# Generate normally distributed error term\n",
    "error = np.random.normal(0, error_std, n_samples)\n",
    "\n",
    "# Calculate outcome variable (Y) based on the theoretical SLR model\n",
    "Y = beta_0 + beta_1 * X + error\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "data = pd.DataFrame({'X': X, 'Y': Y})\n",
    "\n",
    "# Fit the Simple Linear Regression model\n",
    "model = smf.ols('Y ~ X', data=data).fit()\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Plot the observed data, fitted line, and theoretical line\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data['X'], data['Y'], color='blue', label='Observed Data')\n",
    "plt.plot(data['X'], model.predict(data), color='red', label='Fitted Line (Estimated)')\n",
    "plt.plot(data['X'], beta_0 + beta_1 * data['X'], color='green', linestyle='--', label='Theoretical Line (True Model)')\n",
    "plt.xlabel('Predictor (X)')\n",
    "plt.ylabel('Outcome (Y)')\n",
    "plt.title('Comparison of Theoretical and Fitted Simple Linear Regression Lines')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf66f016",
   "metadata": {},
   "source": [
    "# Answer 4\n",
    "\n",
    "In a fitted Simple Linear Regression model, fitted_model.fittedvalues represents the predicted values of the outcome variable \n",
    "Y\n",
    "Y for each observation in the dataset. These values are derived directly from the estimated parameters (intercept and slope) in fitted_model.params or fitted_model.params.values.\n",
    "\n",
    "Deriving fitted_model.fittedvalues\n",
    "The linear regression equation can be written as:\n",
    "\n",
    "Y\n",
    "^\n",
    " =β \n",
    "0\n",
    "​\t\n",
    " +β \n",
    "1\n",
    "​\t\n",
    " X\n",
    "where:\n",
    "\n",
    "Y\n",
    "^\n",
    "  represents the fitted or predicted values (fitted_model.fittedvalues),\n",
    "β\n",
    "0\n",
    "​\t\n",
    "  is the estimated intercept (from fitted_model.params[0]),\n",
    "β\n",
    "1\t\n",
    "  is the estimated slope (from fitted_model.params[1]),\n",
    "X\n",
    "X represents the predictor values for each observation.\n",
    "In statsmodels, fitted_model.params contains these estimated values, which you can also find in fitted_model.summary().tables[1] under the columns for \"coef\" (coefficients). These coefficients are derived from minimizing the residuals, or differences between observed and predicted \n",
    "Y, across the dataset.\n",
    "\n",
    "To calculate fitted_model.fittedvalues, the formula is applied to each observation's \n",
    "X-value with the estimated coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6134c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Simulate data\n",
    "np.random.seed(0)\n",
    "X = np.linspace(0, 10, 100)\n",
    "Y = 5 + 2.5 * X + np.random.normal(0, 1.5, 100)\n",
    "\n",
    "# Create DataFrame and fit model\n",
    "data = pd.DataFrame({'X': X, 'Y': Y})\n",
    "fitted_model = smf.ols('Y ~ X', data=data).fit()\n",
    "\n",
    "# Get estimated parameters\n",
    "intercept = fitted_model.params['Intercept']\n",
    "slope = fitted_model.params['X']\n",
    "\n",
    "# Calculate fitted values manually\n",
    "fitted_values_manual = intercept + slope * data['X']\n",
    "\n",
    "# Compare with fitted_model.fittedvalues\n",
    "print(\"First 5 Fitted Values (Manual Calculation):\", fitted_values_manual.head())\n",
    "print(\"First 5 Fitted Values (fitted_model.fittedvalues):\", fitted_model.fittedvalues.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875bf2ec",
   "metadata": {},
   "source": [
    "# Answer 5\n",
    "\n",
    "In a fitted model using the \"ordinary least squares\" (OLS) method, the line is chosen to minimize the sum of the squared differences between the observed data points and the predicted values on the line. This means that OLS finds the line that best represents the relationship by making the vertical distances (residuals) between observed \n",
    "Y values and their predicted \n",
    "Y\n",
    "^\n",
    "  values as small as possible, on average.\n",
    "\n",
    "The reason it uses \"squares\" is to handle both positive and negative residuals effectively, ensuring they don’t cancel each other out. Squaring also gives more weight to larger residuals, helping to find a line that minimizes large deviations, leading to a better fit overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b1a855",
   "metadata": {},
   "source": [
    "# Answer 6\n",
    "\n",
    "In the context of Simple Linear Regression, the expression for \n",
    "R\n",
    "2\n",
    "R \n",
    "2\n",
    " , or the coefficient of determination, can be interpreted as the proportion of variation in the outcome \n",
    "Y\n",
    "Y that is explained by the model (i.e., the predicted values or fitted_model.fittedvalues).\n",
    "\n",
    "Why \n",
    "R\n",
    "2\n",
    "R \n",
    "2\n",
    "  Measures Proportion of Explained Variation\n",
    "Total Variation in \n",
    "Y\n",
    "Y: The total variation in \n",
    "Y\n",
    "Y is measured by the sum of squared deviations of\n",
    "Y\n",
    "Y from its mean (\n",
    "SST\n",
    "=\n",
    "∑\n",
    "(\n",
    "Y\n",
    "i\n",
    "−\n",
    "Y\n",
    "ˉ\n",
    ")\n",
    "2\n",
    "SST=∑(Y \n",
    "i\n",
    "​\t\n",
    " − \n",
    "Y\n",
    "ˉ\n",
    " ) \n",
    "2\n",
    " ), which represents how spread out the \n",
    "Y\n",
    "Y-values are around their mean.\n",
    "Explained Variation: When we fit a model, it produces predicted values \n",
    "Y\n",
    "^\n",
    "Y\n",
    "^\n",
    "  (or fitted_model.fittedvalues). The variation explained by the model is the sum of squared differences between these predicted values and the mean of \n",
    "Y\n",
    "Y (\n",
    "SSR\n",
    "=\n",
    "∑\n",
    "(\n",
    "Y\n",
    "^\n",
    "i\n",
    "−\n",
    "Y\n",
    "ˉ\n",
    ")\n",
    "2\n",
    "SSR=∑( \n",
    "Y\n",
    "^\n",
    "  \n",
    "i\n",
    "​\t\n",
    " − \n",
    "Y\n",
    "ˉ\n",
    " ) \n",
    "2\n",
    " ).\n",
    "Unexplained Variation: The remaining variation, which the model does not capture, is the sum of squared residuals (\n",
    "SSE\n",
    "=\n",
    "∑\n",
    "(\n",
    "Y\n",
    "i\n",
    "−\n",
    "Y\n",
    "^\n",
    "i\n",
    ")\n",
    "2\n",
    "SSE=∑(Y \n",
    "i\n",
    "​\t\n",
    " − \n",
    "Y\n",
    "^\n",
    "  \n",
    "i\n",
    "​\t\n",
    " ) \n",
    "2\n",
    " ).\n",
    "Proportion of Explained Variation (R-squared): The \n",
    "R\n",
    "2\n",
    "R \n",
    "2\n",
    "  value is defined as:\n",
    "R\n",
    "2\n",
    "=\n",
    "SSR\n",
    "SST\n",
    "=\n",
    "1\n",
    "−\n",
    "SSE\n",
    "SST\n",
    "R \n",
    "2\n",
    " = \n",
    "SST\n",
    "SSR\n",
    "​\t\n",
    " =1− \n",
    "SST\n",
    "SSE\n",
    "​\t\n",
    " \n",
    "This fraction represents the proportion of the total variation in \n",
    "Y\n",
    "Y that is captured by the model. When \n",
    "R\n",
    "2\n",
    "R \n",
    "2\n",
    "  is close to 1, the model explains most of the variation in \n",
    "Y\n",
    "Y; when it is close to 0, it explains very little.\n",
    "Thus, fitted_model.rsquared is a measure of model accuracy, as it shows how well the model fits the data by explaining the observed variation in \n",
    "Y\n",
    "Y. A higher \n",
    "R\n",
    "2\n",
    "R \n",
    "2\n",
    "  indicates a more accurate model in terms of capturing the patterns in the data.\n",
    "\n",
    "Interpretation of \n",
    "np.corrcoef(...)[0,1]\n",
    "2\n",
    "np.corrcoef(...)[0,1] \n",
    "2\n",
    "  in Simple Linear Regression\n",
    "In Simple Linear Regression, \n",
    "R\n",
    "2\n",
    "R \n",
    "2\n",
    "  is also the square of the correlation coefficient between \n",
    "Y\n",
    "Y and\n",
    "Y\n",
    "^\n",
    "Y\n",
    "^\n",
    " :\n",
    "\n",
    "This captures the strength and direction of the linear relationship between the actual and predicted values of \n",
    "Y\n",
    "Y. Squaring the correlation coefficient translates it into a proportion, aligning with \n",
    "R\n",
    "2\n",
    "R \n",
    "2\n",
    " 's role as the proportion of variance explained by the model.\n",
    "\n",
    "Thus:\n",
    "\n",
    "R\n",
    "2\n",
    "R \n",
    "2\n",
    "  (or the squared correlation coefficient) reflects how well the predictor variable \n",
    "X\n",
    "X explains variation in the outcome \n",
    "Y\n",
    "Y.\n",
    "In Simple Linear Regression, it’s both a measure of the fit quality and a gauge of how accurately the model describes the data's linear trend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3dc9a2",
   "metadata": {},
   "source": [
    "# Answer 7\n",
    "\n",
    "In Simple Linear Regression, there are several assumptions about the data that ensure the model's validity and reliability. Based on the example data provided, here are two common assumptions that may not be compatible with the data:\n",
    "\n",
    "Linearity of the Relationship: Simple Linear Regression assumes that there is a linear relationship between the predictor \n",
    "X and the outcome \n",
    "Y. If the scatter plot of the data shows a clear curve or non-linear pattern, this assumption is violated. For instance, if the data points form a U-shape or another curved pattern, a straight line would not adequately capture the relationship, and a different model (e.g., polynomial regression) might be more suitable.\n",
    "Homoscedasticity (Constant Variance of Errors): Another assumption is that the variance of errors (residuals) remains constant across all values of \n",
    "X. This means that the spread of the residuals should be roughly the same throughout the range of \n",
    "X values. If the data points fan out or narrow as \n",
    "X increases or decreases, this indicates heteroscedasticity, meaning the error variance is not constant. In such cases, the model's predictions might be less reliable across the range of \n",
    "X, and transformations or weighted regression might be necessary.\n",
    "Without seeing the actual data plot, these are common reasons Simple Linear Regression assumptions may not hold. If you have a plot or specific observations about the data, I can give more targeted feedback on other assumptions, such as normality or independence of errors.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad0194b",
   "metadata": {},
   "source": [
    "# Answer 8\n",
    "\n",
    "To evaluate whether there is a linear association between waiting time (the time between eruptions) and duration (the length of each eruption) in the Old Faithful Geyser dataset, we can set up and test a null hypothesis within the framework of Simple Linear Regression.\n",
    "\n",
    "Null Hypothesis\n",
    "The null hypothesis (\n",
    "H \n",
    "0\n",
    "​\t\n",
    " ) in this case is:\n",
    "\n",
    "H \n",
    "0\n",
    "​\t\n",
    " :β \n",
    "1\n",
    "​\t\n",
    " =0 — There is no linear association between waiting time and eruption duration (i.e., the slope of the regression line is zero).\n",
    "If \n",
    "0\n",
    "β \n",
    "1\n",
    "​\t\n",
    " =0, then changes in waiting time would not predict any systematic change in eruption duration, suggesting no linear relationship between the variables.\n",
    "\n",
    "Code for Fitting and Testing the Model\n",
    "We’ll fit a Simple Linear Regression model to the data, then use the results to test the null hypothesis by examining the p-value associated with the slope coefficient (\n",
    "β\n",
    "1\t\n",
    " )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884488c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Fit Simple Linear Regression model: duration ~ waiting\n",
    "model = smf.ols('duration ~ waiting', data=old_faithful).fit()\n",
    "\n",
    "# Output the summary of the model, which includes p-values for hypothesis tests\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff12d0ea",
   "metadata": {},
   "source": [
    "# Answer 9\n",
    "\n",
    "To analyze the evidence for a relationship between eruption duration and wait time using only short wait times, we can restrict the dataset to those observations where the waiting time is less than specified limits (62, 64, and 66 minutes). We will then fit a Simple Linear Regression model for each subset and evaluate the null hypothesis \n",
    "H \n",
    "0\n",
    "​\t\n",
    " :β \n",
    "1\n",
    "​\t\n",
    " =0 for each case.\n",
    "\n",
    "Steps to Analyze Short Wait Times\n",
    "\n",
    "1. Filter the Dataset: For each wait time limit, we will filter the dataset to include only those rows where the waiting time is below the specified threshold.\n",
    "\n",
    "2. Fit the Simple Linear Regression Model: For each filtered dataset, we will fit the regression model.\n",
    "\n",
    "3. Examine the Results: We will extract the p-value associated with the slope to assess whether we can reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b1fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# Load the Old Faithful Geyser dataset\n",
    "old_faithful = sns.load_dataset('geyser')\n",
    "\n",
    "# Define short wait time limits\n",
    "short_wait_limits = [62, 64, 66]\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "for limit in short_wait_limits:\n",
    "    # Filter the dataset for wait times less than the limit\n",
    "    short_wait_data = old_faithful[old_faithful['waiting'] < limit]\n",
    "    \n",
    "    # Fit the Simple Linear Regression model: duration ~ waiting\n",
    "    model = smf.ols('duration ~ waiting', data=short_wait_data).fit()\n",
    "    \n",
    "    # Store the summary statistics, including p-value for the slope\n",
    "    results[limit] = {\n",
    "        'p-value': model.pvalues['waiting'],\n",
    "        'R-squared': model.rsquared,\n",
    "        'model_summary': model.summary()\n",
    "    }\n",
    "\n",
    "# Print the results for each limit\n",
    "for limit, result in results.items():\n",
    "    print(f\"Short Wait Limit: {limit} minutes\")\n",
    "    print(f\"  P-value for slope: {result['p-value']}\")\n",
    "    print(f\"  R-squared: {result['R-squared']}\")\n",
    "    print(result['model_summary'])\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a9fd50",
   "metadata": {},
   "source": [
    "Interpreting the Results\n",
    "\n",
    "1. P-Value for Slope: For each of the models fit to the data with short wait times, we check the p-value associated with the slope (waiting time).\n",
    "If the p-value is less than 0.05: We reject the null hypothesis, suggesting that there is evidence of a linear relationship between waiting time and eruption duration even within the shorter wait time context.\n",
    "If the p-value is greater than 0.05: We fail to reject the null hypothesis, indicating insufficient evidence of a linear relationship between waiting time and eruption duration in this subset.\n",
    "\n",
    "2. R-squared Value: This statistic gives an indication of how much variance in eruption duration is explained by the waiting time. A higher \n",
    "R \n",
    "2\n",
    "  value suggests a better fit for the linear model within that subset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34456288",
   "metadata": {},
   "source": [
    "# Answer 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beb1bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Old Faithful Geyser dataset\n",
    "old_faithful = sns.load_dataset('geyser')\n",
    "\n",
    "# Filter for long wait times (assuming \"long_wait\" corresponds to some condition)\n",
    "long_wait = old_faithful[old_faithful['waiting'] >= 63]  # Adjust based on your definition of long wait\n",
    "\n",
    "# Step 1: Create bootstrapped Simple Linear Regression models\n",
    "n_bootstrap_samples = 1000\n",
    "bootstrapped_slopes = []\n",
    "\n",
    "for _ in range(n_bootstrap_samples):\n",
    "    # Sample with replacement\n",
    "    bootstrap_sample = long_wait.sample(n=len(long_wait), replace=True)\n",
    "    model = smf.ols('duration ~ waiting', data=bootstrap_sample).fit()\n",
    "    bootstrapped_slopes.append(model.params['waiting'])\n",
    "\n",
    "# Step 2: Visualize the bootstrapped sampling distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(bootstrapped_slopes, bins=30, kde=True)\n",
    "plt.title('Bootstrapped Sampling Distribution of Slope Coefficients')\n",
    "plt.xlabel('Slope Coefficient')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(np.mean(bootstrapped_slopes), color='red', linestyle='dashed', linewidth=2, label='Mean Slope')\n",
    "plt.axvline(np.percentile(bootstrapped_slopes, 2.5), color='blue', linestyle='dashed', linewidth=2, label='95% CI Lower Bound')\n",
    "plt.axvline(np.percentile(bootstrapped_slopes, 97.5), color='blue', linestyle='dashed', linewidth=2, label='95% CI Upper Bound')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Simulate samples under the null hypothesis\n",
    "n_simulations = 1000\n",
    "simulated_slopes = []\n",
    "b0 = 1.65  # Intercept\n",
    "sigma = 0.37  # Standard deviation\n",
    "waiting_values = np.linspace(long_wait['waiting'].min(), long_wait['waiting'].max(), n=160)  # Simulated waiting times\n",
    "\n",
    "for _ in range(n_simulations):\n",
    "    # Generate errors\n",
    "    errors = np.random.normal(0, sigma, size=waiting_values.shape)\n",
    "    # Simulated durations based on the null hypothesis\n",
    "    simulated_durations = b0 + errors\n",
    "    simulated_data = pd.DataFrame({'waiting': waiting_values, 'duration': simulated_durations})\n",
    "    sim_model = smf.ols('duration ~ waiting', data=simulated_data).fit()\n",
    "    simulated_slopes.append(sim_model.params['waiting'])\n",
    "\n",
    "# Step 4: Visualize the sampling distribution of the simulated slopes\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(simulated_slopes, bins=30, kde=True)\n",
    "plt.title('Simulated Sampling Distribution of Slope Coefficients (Null Hypothesis)')\n",
    "plt.xlabel('Slope Coefficient')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(np.mean(simulated_slopes), color='red', linestyle='dashed', linewidth=2, label='Mean Slope')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Calculate the 95% confidence interval for the bootstrapped slopes\n",
    "ci_lower = np.percentile(bootstrapped_slopes, 2.5)\n",
    "ci_upper = np.percentile(bootstrapped_slopes, 97.5)\n",
    "\n",
    "# Check if the simulated slope under the null hypothesis (which is 0) is in the CI\n",
    "slope_under_null = 0\n",
    "is_contained = ci_lower <= slope_under_null <= ci_upper\n",
    "\n",
    "# Step 6: Compare simulated p-value with the original model's p-value\n",
    "original_model = smf.ols('duration ~ waiting', data=long_wait).fit()\n",
    "original_p_value = original_model.pvalues['waiting']\n",
    "\n",
    "# Simulated p-value\n",
    "simulated_p_value = np.mean(np.array(simulated_slopes) >= original_model.params['waiting'])\n",
    "\n",
    "# Results\n",
    "print(f\"95% Bootstrapped Confidence Interval: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "print(f\"Slope under null hypothesis (0) is contained in CI: {is_contained}\")\n",
    "print(f\"Original model's p-value: {original_p_value:.4f}\")\n",
    "print(f\"Simulated p-value: {simulated_p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed607fbd",
   "metadata": {},
   "source": [
    "Reporting Results\n",
    "\n",
    "After running the code, we'll get a report indicating:\n",
    "\n",
    "1. The 95% bootstrapped confidence interval for the slope.\n",
    "\n",
    "2. Whether the null hypothesis slope is contained within that interval.\n",
    "\n",
    "3. The original model’s p-value and the simulated p-value to compare the significance of the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2aacd7",
   "metadata": {},
   "source": [
    "# Answer 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fd9ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for wait time category\n",
    "old_faithful['wait_time_category'] = np.where(old_faithful['waiting'] < 68, 'short', 'long')\n",
    "\n",
    "# Fit the new model using the indicator variable\n",
    "model_category = smf.ols('duration ~ wait_time_category', data=old_faithful).fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9381d0",
   "metadata": {},
   "source": [
    "Big Picture Differences\n",
    "\n",
    "Indicator Variable vs. Continuous Variable:\n",
    "\n",
    "1. Previous Specifications: The earlier models were built using the waiting time as a continuous variable. They aimed to explore a linear relationship where changes in waiting time directly correspond to changes in eruption duration. The focus was on estimating a single slope for the entire dataset.\n",
    "\n",
    "2. New Specification: By using an indicator variable, the model distinguishes between two groups (\"short\" and \"long\") without assuming a linear relationship within those groups. Instead, it estimates separate means for eruption durations based on the category of wait time.\n",
    "\n",
    "Interpretation of Coefficients:\n",
    "\n",
    "3. Previous Models: The coefficient for waiting time represented the expected change in eruption duration for each additional minute of waiting time.\n",
    "\n",
    "4. New Model: The coefficients for the indicator variable represent the expected difference in eruption duration between the two groups (short vs. long wait times). This model effectively answers the question: \"How does eruption duration differ between short and long wait times?\"\n",
    "\n",
    "Flexibility in Relationships:\n",
    "\n",
    "5. Previous Models: The assumption was that the relationship between waiting time and duration was consistent across all observations. If the actual relationship is not linear (e.g., a step function or different means), this could lead to incorrect conclusions.\n",
    "\n",
    "6. New Model: By segmenting the data, we allow for different average durations for the two groups. This can be particularly useful if the impact of waiting time on eruption duration differs qualitatively between short and long waits.\n",
    "\n",
    "Hypothesis Testing\n",
    "We can set up the null hypothesis for the new model:\n",
    "\n",
    "Null Hypothesis (\n",
    "H \n",
    "0\n",
    "​\t\n",
    " ): There is no difference in eruption duration between short and long wait times (i.e., the mean eruption duration for short waits equals that for long waits).\n",
    "Alternative Hypothesis (\n",
    "H \n",
    "a\n",
    "​\t\n",
    " ): There is a difference in eruption duration between short and long wait times (i.e., the means are not equal).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e226a45f",
   "metadata": {},
   "source": [
    "# Answer 12\n",
    "\n",
    "Histograms of Residuals for Each Model\n",
    "\n",
    "1. Model 1: Simple Linear Regression using Continuous Waiting Time\n",
    "\n",
    "Residuals Histogram: The histogram may reveal a distribution that approximates normality, but it could also show skewness or kurtosis, indicating departures from the normal distribution.\n",
    "\n",
    "2. Model 2: Simple Linear Regression using Short Wait Times (<68)\n",
    "\n",
    "Residuals Histogram: Depending on the distribution of durations for short wait times, this histogram may show skewness or have a peaked distribution, suggesting that the assumption of normality may not hold.\n",
    "\n",
    "3. Model 3: Simple Linear Regression using Long Wait Times (>68)\n",
    "\n",
    "Residuals Histogram: Similar to Model 2, this histogram could either support or contradict the normality assumption, depending on how eruption durations are distributed among long wait times.\n",
    "\n",
    "4. Model 4: Simple Linear Regression with Indicator Variable (Short vs. Long Wait)\n",
    "\n",
    "Residuals Histogram: This model's histogram may suggest normality if the residuals are symmetrically distributed around zero without excessive skewness or kurtosis.\n",
    "\n",
    "Identifying Support for Normality\n",
    "\n",
    "To determine which histogram suggests normality and which do not, consider the following characteristics:\n",
    "\n",
    "1. Support for Normality:\n",
    "Symmetric Shape: If one histogram appears bell-shaped and symmetric about the mean, it is a strong indicator of normality.\n",
    "\n",
    "Lack of Skewness: If the histogram does not extend more on one side than the other, it supports the normality assumption.\n",
    "\n",
    "Light Tails: A histogram that does not have heavy tails or extreme outliers also suggests normality.\n",
    "\n",
    "2. Lack of Support for Normality:\n",
    "\n",
    "Skewness: If a histogram has a tail on one side (either left or right), it suggests that the errors are not normally distributed.\n",
    "\n",
    "Bimodality: If the histogram shows two distinct peaks, it indicates that there may be two different processes at work, violating the assumption of normality.\n",
    "\n",
    "Heavy Tails: If the histogram displays extreme values far from the center, it suggests that the error terms are not normally distributed and may follow a different distribution (e.g., exponential or Cauchy).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e4045b",
   "metadata": {},
   "source": [
    "# Answer 13\n",
    "\n",
    "(A) Permutation Test\n",
    "\n",
    "The permutation test is a non-parametric method used to determine whether there is a significant difference between the means of two groups. The basic idea is to shuffle the labels of the groups and calculate the difference in means repeatedly to create a distribution of the test statistic under the null hypothesis.\n",
    "\n",
    "Steps for Permutation Test:\n",
    "\n",
    "1. Calculate the Observed Difference in Means: Compute the mean duration for both \"short\" and \"long\" wait times and find the difference.\n",
    "\n",
    "2. Combine the Data: Merge the durations from both groups into a single dataset.\n",
    "\n",
    "3. Shuffle the Labels: Randomly shuffle the group labels and reassign them to the combined dataset.\n",
    "\n",
    "4. Calculate New Differences: For each permutation, calculate the mean difference between the shuffled groups.\n",
    "\n",
    "5. Repeat: Repeat the shuffle and calculation many times (e.g., 10,000 iterations) to build a distribution of mean differences under the null hypothesis.\n",
    "\n",
    "6. Determine Significance: Compare the observed difference to the permutation distribution to determine how extreme it is. This can be done by calculating a p-value based on the proportion of permuted differences that are as extreme as or more extreme than the observed difference.\n",
    "\n",
    "(B) 95% Bootstrap Confidence Interval\n",
    "\n",
    "The bootstrap method involves resampling with replacement to create an empirical distribution of the sample mean differences.\n",
    "\n",
    "Steps for Bootstrap Confidence Interval:\n",
    "\n",
    "1. Calculate the Observed Mean Difference: Compute the mean eruption duration for both groups and calculate the difference.\n",
    "\n",
    "2. Resampling: For each group, repeatedly sample with replacement to create new bootstrap samples.\n",
    "\n",
    "3. Calculate Bootstrap Mean Differences: For each bootstrap iteration, calculate the mean for both groups and then compute the difference between these means.\n",
    "\n",
    "4. Collect Differences: Store all calculated mean differences from each bootstrap sample.\n",
    "\n",
    "5. Construct Confidence Interval: Use np.quantile to find the 2.5th and 97.5th percentiles of the collection of mean differences to construct the 95% bootstrap confidence interval.\n",
    "\n",
    "(a) Explanation of Sampling Approaches\n",
    "\n",
    "Permutation Test: This method assesses the null hypothesis by randomizing group labels, thus creating a distribution of differences assuming no real effect. It is particularly useful when the sample sizes are small or the underlying distributions are unknown, as it does not rely on assumptions of normality.\n",
    "\n",
    "Bootstrap Confidence Interval: This approach estimates the uncertainty of the difference in means by creating an empirical distribution through resampling. It allows us to quantify the variability and confidence in the observed mean difference without assuming a particular distribution for the data.\n",
    "\n",
    "Both methods are powerful tools for hypothesis testing and confidence interval estimation, providing flexibility and robustness in the face of data variability and non-normality.\n",
    "\n",
    "(b) Comparison with the Indicator Variable-Based Model\n",
    "\n",
    "Similarities:\n",
    "\n",
    "1. Non-parametric Nature: Both the permutation test and bootstrap methods do not assume a specific distribution for the data, similar to the indicator variable model that compares group means without strict linearity assumptions.\n",
    "\n",
    "2. Focus on Group Differences: All three approaches seek to understand differences between the \"short\" and \"long\" wait times in terms of eruption durations.\n",
    "\n",
    "Differences:\n",
    "\n",
    "1. Approach to Hypothesis Testing:\n",
    "\n",
    "Indicator Variable Model: It estimates the mean durations directly and tests the hypothesis by evaluating coefficients in a regression framework. It uses t-tests to assess the significance of differences.\n",
    "\n",
    "Permutation Test: Directly evaluates the null hypothesis by generating a distribution of the mean differences through randomization, making no assumptions about the data's distribution.\n",
    "\n",
    "Bootstrap Confidence Interval: Instead of hypothesis testing, it focuses on estimating the range of plausible values for the difference in means through empirical resampling.\n",
    "\n",
    "2. Interpretability:\n",
    "\n",
    "Indicator Variable Model: Provides clear coefficients that represent group means and their differences, which are easy to interpret within the context of regression analysis.\n",
    "\n",
    "Permutation and Bootstrap Methods: Focus on statistical evidence for differences and variability, which may require additional interpretation of results to communicate findings effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3510024",
   "metadata": {},
   "source": [
    "# Answer 14\n",
    "\n",
    "yes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8db127b",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Summary of Key Points\n",
    "\n",
    "1. Simple Linear Regression Overview:\n",
    "\n",
    "Discussed the components of a simple linear regression model, including predictor and outcome variables, slope and intercept coefficients, and the error term.\n",
    "\n",
    "Explained how these components relate to the normal distribution of error terms.\n",
    "\n",
    "2. Visualization and Analysis:\n",
    "\n",
    "Used the Old Faithful dataset to create scatter plots with trendlines for visual analysis.\n",
    "\n",
    "Explained how fitted values are derived from model parameters and how the ordinary least squares method selects the best-fitting line.\n",
    "\n",
    "3. Model Evaluation:\n",
    "Discussed the interpretation of \n",
    "R\n",
    "2\n",
    "  and the correlation coefficient, emphasizing their roles in assessing model accuracy.\n",
    "  \n",
    "Identified assumptions of the simple linear regression model that may not align with the dataset, such as normality of residuals.\n",
    "\n",
    "4. Hypothesis Testing Approaches:\n",
    "\n",
    "Explained the use of permutation tests and bootstrap methods to assess the differences in eruption durations between short and long wait times.\n",
    "\n",
    "Permutation tests shuffle group labels to create a distribution of mean differences under the null hypothesis, while bootstrap methods resample data to estimate confidence intervals for mean differences.\n",
    "\n",
    "5. Comparison with Indicator Variable Model:\n",
    "\n",
    "Discussed the similarities and differences between the hypothesis testing methods and the indicator variable-based regression model.\n",
    "\n",
    "Emphasized that while both approaches focus on differences between groups, the indicator variable model provides a clearer interpretation of group means and their differences, while the other methods focus on statistical evidence for those differences.\n",
    "\n",
    "6. Statistical Significance and Interpretation:\n",
    "\n",
    "Highlighted how to determine the significance of differences between groups using p-values and confidence intervals derived from the permutation and bootstrap methods.\n",
    "\n",
    "Discussed how these methods provide flexibility and robustness in statistical analysis, particularly when dealing with non-normal data distributions.\n",
    "\n",
    "This session covered a range of statistical concepts, methods, and their applications to real-world data, emphasizing the importance of understanding model assumptions and evaluating differences between groups using various techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ecdbde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
